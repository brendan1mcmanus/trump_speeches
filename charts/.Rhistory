help("array-class")
ggplot(combinedData, aes(x=combinedData$Date, y=combinedData$value, group=combinedData$item_code)) + geom_line() + theme_classic()
library(rjson)
library(blsAPI)
library(ggplot2)
library(reshape2)
currentData <- read.table(
"All Urban Consumers Current Series/cu.data.0.Current.txt",
sep="\t", header=TRUE)
seriesHeaders <- read.table(
"All Urban Consumers Current Series/cu.series.txt",
sep="\t", header=TRUE)
head(seriesHeaders)
head(currentData)
months <- read.table("All Urban Consumers Current Series/cu.period.txt", sep = "\t", header=TRUE)
prices <- read.table("All Urban Consumers Current Series/cu.item.txt", sep = "\t", header=TRUE)
prices[,c("item_code")]
str(months)
str(prices)
prices[prices$display_level == TRUE, ]
head(prices)
combinedData <- merge(currentData, seriesHeaders, by="series_id")
combinedData <- merge(combinedData, months, by="period")
combinedData <- merge(combinedData, prices, by="item_code")
head(combinedData)
combinedData$Date <- paste( combinedData$period_name , "1" , combinedData$year , sep = " ")
head(combinedData)
combinedData$Date <- as.Date(combinedData$Date, format = "%B %d %Y")
head(combinedData)
df_unique <- unique(combinedData$item_code)
length(df_unique)
newData <- combinedData[,c("Date", "value", "item_code")]
saodata <- newData[newData$item_code == "SA0",]
ggplot(saodata, aes(x=saodata$Date, y=saodata$value)) + geom_line() + theme_classic()
cleanedData <- filter(newData, newData$item_code == "SA0")
head(cleanedData)
refinedData <- dcast(cleanedData, cleanedData$Date ~ cleanedData$item_code, value.var = cleanedData$value)
head(refinedData)
## Read in percentage changes for each category item_code
ggplot(combinedData, aes(x=combinedData$Date, y=combinedData$value, group=combinedData$item_code)) + geom_line() + theme_classic()
head(currentData)
head(refinedData)
newData <- combinedData[,c("Date", "value", "item_code")]
saodata <- newData[newData$item_code == "SA0",]
ggplot(saodata, aes(x=saodata$Date, y=saodata$value)) + geom_line() + theme_classic()
cleanedData <- filter(newData, newData$item_code == "SA0")
head(cleanedData)
refinedData <- dcast(cleanedData, cleanedData$Date ~ cleanedData$item_code, value.var = cleanedData$value)
head(refinedData)
newData <- combinedData[,c("Date", "value", "item_code")]
saodata <- newData[newData$item_code == "SA0",]
ggplot(saodata, aes(x=saodata$Date, y=saodata$value)) + geom_line() + theme_classic()
months <- read.table("All Urban Consumers Current Series/cu.period.txt", sep = "\t", header=TRUE)
prices <- read.table("All Urban Consumers Current Series/cu.item.txt", sep = "\t", header=TRUE)
# Define the packages we want to load:
packs = c(
"tm",                         # Text mining
"tm.plugin.webmining",        # Web-source plugin for text mining
"SnowballC",                  # Stemmer
"RColorBrewer",               # Colors for visualisation
"ggplot2",                    # Plotting
"wordcloud",                  # Draw wordclouds
"openNLP",                     # Split text into sentences.
"gtools"
)
sapply(packs, require, character.only=TRUE)   # Load the packages.
# Define the packages we want to load:
packs = c(
"tm",                         # Text mining
"tm.plugin.webmining",        # Web-source plugin for text mining
"SnowballC",                  # Stemmer
"RColorBrewer",               # Colors for visualisation
"ggplot2",                    # Plotting
"wordcloud",                  # Draw wordclouds
"openNLP",                     # Split text into sentences.
"gtools"
)
sapply(packs, require, character.only=TRUE)   # Load the packages.
setwd(dir = "~/Desktop/dev/news_analysis/martinchek-2012-2016-facebook-posts/")
setwd("~/Desktop/dev/trump_speeches/hillary_data//")
hillary_txt_files = list.files(pattern = 'speech_')
hillary_txt_files = mixedsort(hillary_txt_files)
ToSentences = function(text, language="en") {
# Splits text into sentences using an Apache OpenNLP sentence detector.
# Arguments:
# "text" the text to be processed (character)
# "lang" ISO-639 code of the language of the text (character)
# Returns:
# sentences of the text (character vector)
if(length(text) ==0)      {return("")}
if(nchar(text) == 0)   {return("")}   # Cover special case 0-character text.
# Convert text to String object; allows for splitting by index.
text = as.String(text)
# Discover the sentence markers in the text (specify NLP as
# source of annotate because there is also an annotate function in ggplot2)
markers = NLP::annotate(
text,
Maxent_Sent_Token_Annotator(language=language)   # Annotator from OpenNLP
)
# Return sentences by splitting the text at the boundaries.
text[markers]
}
CorpusToSentences = function(corpus) {
# Split every document in the corpus into sentences and return a new corpus
# with all the sentences as individual documents.
# Extract the text from each document in the corpus.
#text = lapply(corpus, "[[", "content")
# Basically convert the text
docs = lapply(corpus, ToSentences)
docs = as.vector(unlist(docs))
# Return a corpus with sentences as documents.
Corpus(VectorSource(docs))
}
corpora = lapply(hillary_txt_files, readLines)
warnings()
e
e
hillary_txt_files = list.files(pattern = 'speech_')
hillary_txt_files = mixedsort(hillary_txt_files)
corpora = lapply(hillary_txt_files, readLines)
BreakDownCorpus = function(corpora) {
# Create a new corpus which merges existing corpora after splitting them
# into sentences.
corpus = Reduce(c, lapply(corpora, CorpusToSentences))
# Process the corpora contents.
corpus = tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
corpus
}
corpus = BreakDownCorpus(corpora)
dtm = DocumentTermMatrix(corpus)
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
setwd("~/Desktop/dev/trump_speeches/")
lex = read.csv("inquirerbasic.csv", stringsAsFactors=FALSE)
lex$Entry = gsub("#1", "", lex$Entry)
lex = lex[!grepl("#", lex$Entry), ]
neg.lex = tolower(lex$Entry[lex$Negativ != ""])
pos.lex = tolower(lex$Entry[lex$Positiv != ""])
terms = colnames(dtm)
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
neg.scores = rowSums(as.matrix(dtm[ , neg.terms]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
document.signs = sign(document.scores)
sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
PosCloud = function() {
wordcloud(
pos.terms,
colSums(as.matrix(dtm[ , pos.terms])),
min.freq=1,
scale=c(4,0.7),
color=brewer.pal(n=9, "Blues")[6:9]
)
}
NegCloud = function() {
wordcloud(
neg.terms,
colSums(as.matrix(dtm[ , neg.terms])),
min.freq=1,
scale=c(4,0.7),
color=brewer.pal(n=9, "Reds")[6:9]
)
}
height = 8
width = 8
setwd("~/Desktop/dev/trump_speeches/analysis/")
pdf("HillaryPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
warnings()
pdf("HillaryNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
pdf("HillaryPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
height = 12
width = 12
pdf("HillaryPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("HillaryNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
height = 9
width = 9
pdf("HillaryPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("HillaryNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
setwd("~/Desktop/dev/trump_speeches/trump_data//")
txt_files = list.files(pattern = 'speech_')
txt_files = mixedsort(txt_files)
corpora = lapply(txt_files, readLines)
trump_txt_files = list.files(pattern = 'speech_')
trump_txt_files = mixedsort(txt_files)
corpora = lapply(trump_txt_files, readLines)
corpus = BreakDownCorpus(corpora)
dtm = DocumentTermMatrix(corpus)
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
setwd("~/Desktop/dev/trump_speeches/")
terms = colnames(dtm)
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
neg.scores = rowSums(as.matrix(dtm[ , neg.terms]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
document.signs = sign(document.scores)
trump.sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
height = 9
width = 9
setwd("~/Desktop/dev/trump_speeches/analysis/")
pdf("TrumpPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("TrumpNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
corpora = lapply(hillary_txt_files, readLines)
corpus = BreakDownCorpus(corpora)
dtm = DocumentTermMatrix(corpus)
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
terms = colnames(dtm)
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
# Specify positive terms which may be quiestionable.
pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
neg.terms.adj = setdiff(neg.terms, c("need"))
# Calculate the negative and positive sentence scores ("document scores").
neg.scores = rowSums(as.matrix(dtm[ , neg.terms]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
# Calulate the document signs ("sentence signs").
document.signs = sign(document.scores)
# Calculate the sentiment score
hillary.sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
setwd("~/Desktop/dev/trump_speeches/hillary_data/")
hillary_txt_files = list.files(pattern = 'speech_')
hillary_txt_files = mixedsort(hillary_txt_files)
corpora = lapply(hillary_txt_files, readLines)
corpus = BreakDownCorpus(corpora)
dtm = DocumentTermMatrix(corpus)
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
terms = colnames(dtm)
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
# Specify positive terms which may be quiestionable.
pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
neg.terms.adj = setdiff(neg.terms, c("need"))
# Calculate the negative and positive sentence scores ("document scores").
neg.scores = rowSums(as.matrix(dtm[ , neg.terms]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
# Calulate the document signs ("sentence signs").
document.signs = sign(document.scores)
# Calculate the sentiment score
hillary.sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
height = 9
width = 9
setwd("~/Desktop/dev/trump_speeches/analysis/")
pdf("HillaryPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
hillary.sentiment.score
trump.sentiment.score
neg.terms
neg.lex
neg.terms.adj = setdiff(neg.terms, c("need"))
neg.terms.adj
trump_txt_files = list.files(pattern = 'speech_')
trump_txt_files = mixedsort(txt_files)
corpora = lapply(trump_txt_files, readLines)
corpus = BreakDownCorpus(corpora)
dtm = DocumentTermMatrix(corpus)
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
terms = colnames(dtm)
# Find the positive and negative terms using the lexicons.
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
# Specify positive terms which may be quiestionable.
neg.terms.adj = setdiff(neg.terms, c("need"))
# Calculate the negative and positive sentence scores ("document scores").
neg.scores = rowSums(as.matrix(dtm[ , neg.terms]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
# Calulate the document signs ("sentence signs").
document.signs = sign(document.scores)
# Calculate the sentiment score
trump.sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
setwd("~/Desktop/dev/trump_speeches/trump_data/")
trump_txt_files = list.files(pattern = 'speech_')
trump_txt_files = mixedsort(txt_files)
corpora = lapply(trump_txt_files, readLines)
corpus = BreakDownCorpus(corpora)
dtm = DocumentTermMatrix(corpus)
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
terms = colnames(dtm)
# Find the positive and negative terms using the lexicons.
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
# Specify positive terms which may be quiestionable.
neg.terms.adj = setdiff(neg.terms, c("need"))
# Calculate the negative and positive sentence scores ("document scores").
neg.scores = rowSums(as.matrix(dtm[ , neg.terms]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
# Calulate the document signs ("sentence signs").
document.signs = sign(document.scores)
# Calculate the sentiment score
trump.sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
pdf("TrumpPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("TrumpNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
setwd("~/Desktop/dev/trump_speeches/charts/")
pdf("TrumpPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("TrumpNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
neg.terms
neg.scores
neg.terms
neg.terms.adj = setdiff(neg.terms, c("need"))
setwd("~/Desktop/dev/trump_speeches/charts/")
pdf("TrumpPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("TrumpNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
neg.terms.adj
neg.terms
neg.lex
setdiff(neg.lex, c("need"))
neg.lex = setdiff(neg.lex, c("need"))
setwd("~/Desktop/dev/trump_speeches/trump_data/")
trump_txt_files = list.files(pattern = 'speech_')
trump_txt_files = mixedsort(txt_files)
corpora = lapply(trump_txt_files, readLines)
corpus = BreakDownCorpus(corpora)
dtm = DocumentTermMatrix(corpus)
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
terms = colnames(dtm)
# Find the positive and negative terms using the lexicons.
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
# Specify positive terms which may be questionable.
neg.terms.adj = setdiff(neg.terms, c("need"))
# Calculate the negative and positive sentence scores ("document scores").
neg.scores = rowSums(as.matrix(dtm[ , neg.terms]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
# Calulate the document signs ("sentence signs").
document.signs = sign(document.scores)
# Calculate the sentiment score
trump.sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
setwd("~/Desktop/dev/trump_speeches/charts/")
pdf("TrumpPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("TrumpNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
setwd("~/Desktop/dev/trump_speeches/hillary_data/")
hillary_txt_files = list.files(pattern = 'speech_')
hillary_txt_files = mixedsort(hillary_txt_files)
corpora = lapply(hillary_txt_files, readLines)
corpus = BreakDownCorpus(corpora)
dtm = DocumentTermMatrix(corpus)
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
terms = colnames(dtm)
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
# Specify positive terms which may be quiestionable.
neg.terms.adj = setdiff(neg.terms, c("need"))
# Calculate the negative and positive sentence scores ("document scores").
neg.scores = rowSums(as.matrix(dtm[ , neg.terms.adj]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
# Calulate the document signs ("sentence signs").
document.signs = sign(document.scores)
# Calculate the sentiment score
hillary.sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
height = 9
width = 9
setwd("~/Desktop/dev/trump_speeches/charts//")
pdf("HillaryPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("HillaryNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
pos.lex = setdiff(pos.lex, c("applause"))
setwd("~/Desktop/dev/trump_speeches/trump_data/")
trump_txt_files = list.files(pattern = 'speech_')
trump_txt_files = mixedsort(txt_files)
corpora = lapply(trump_txt_files, readLines)
corpus = BreakDownCorpus(corpora)
dtm = DocumentTermMatrix(corpus)
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
terms = colnames(dtm)
# Find the positive and negative terms using the lexicons.
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
# Specify positive terms which may be questionable.
neg.terms.adj = setdiff(neg.terms, c("need"))
# Calculate the negative and positive sentence scores ("document scores").
neg.scores = rowSums(as.matrix(dtm[ , neg.terms]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
# Calulate the document signs ("sentence signs").
document.signs = sign(document.scores)
# Calculate the sentiment score
trump.sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
setwd("~/Desktop/dev/trump_speeches/charts/")
pdf("TrumpPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("TrumpNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
setwd("~/Desktop/dev/trump_speeches/hillary_data/")
hillary_txt_files = list.files(pattern = 'speech_')
hillary_txt_files = mixedsort(hillary_txt_files)
corpora = lapply(hillary_txt_files, readLines)
corpus = BreakDownCorpus(corpora)
dtm = DocumentTermMatrix(corpus)
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
terms = colnames(dtm)
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
# Specify positive terms which may be quiestionable.
neg.terms.adj = setdiff(neg.terms, c("need"))
# Calculate the negative and positive sentence scores ("document scores").
neg.scores = rowSums(as.matrix(dtm[ , neg.terms.adj]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
# Calulate the document signs ("sentence signs").
document.signs = sign(document.scores)
# Calculate the sentiment score
hillary.sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
height = 9
width = 9
setwd("~/Desktop/dev/trump_speeches/charts/")
pdf("HillaryPositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("HillaryNegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
hillary.sentiment.score
