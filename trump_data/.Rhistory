# Define the packages we want to load:
packs = c(
"tm",                         # Text mining
"tm.plugin.webmining",        # Web-source plugin for text mining
"SnowballC",                  # Stemmer
"RColorBrewer",               # Colors for visualisation
"ggplot2",                    # Plotting
"wordcloud",                  # Draw wordclouds
"openNLP"                     # Split text into sentences.
)
sapply(packs, require, character.only=TRUE)   # Load the packages.
}
ToSentences = function(text, language="en") {
# Splits text into sentences using an Apache OpenNLP sentence detector.
# Arguments:
# "text" the text to be processed (character)
# "lang" ISO-639 code of the language of the text (character)
# Returns:
# sentences of the text (character vector)
if(length(text) ==0)      {return("")}
if(nchar(text) == 0)   {return("")}   # Cover special case 0-character text.
# Convert text to String object; allows for splitting by index.
text = as.String(text)
# Discover the sentence markers in the text (specify NLP as
# source of annotate because there is also an annotate function in ggplot2)
markers = NLP::annotate(
text,
Maxent_Sent_Token_Annotator(language=language)   # Annotator from OpenNLP
)
# Return sentences by splitting the text at the boundaries.
text[markers]
}
sapply(packs, require, character.only=TRUE)   # Load the packages.
install.packages("tm.plugin.webmining")
install.packages("SnowballC")
install.packages("wordcloud")
sapply(packs, require, character.only=TRUE)   # Load the packages.
corpus = lapply(txt_files, readLines)
corpus = Reduce(c, lapply(corpora, CorpusToSentences))
txt_files = list.files(pattern = 'speech_')
corpus = lapply(txt_files, readLines)
warnings()
txt_files = list.files(pattern = 'speech_')
corpus = lapply(txt_files, readLines)
warnings()
txt_files = list.files(pattern = 'speech_')
ToSentences = function(text, language="en") {
corpus = lapply(txt_files, readLines)
corpus = lapply(txt_files, readLines)
setwd("~/Desktop/dev/trump_speeches/data/")
txt_files = list.files(pattern = 'speech_')
corpus = lapply(txt_files, readLines)
corpus = Reduce(c, lapply(corpora, CorpusToSentences))
CorpusToSentences = function(corpus) {
# Split every document in the corpus into sentences and return a new corpus
# with all the sentences as individual documents.
# Extract the text from each document in the corpus.
text = lapply(corpus, "[[", "content")
# Basically convert the text
docs = lapply(text, ToSentences)
docs = as.vector(unlist(docs))
# Return a corpus with sentences as documents.
Corpus(VectorSource(docs))
}
corpus = Reduce(c, lapply(corpora, CorpusToSentences))
corpora = lapply(txt_files, readLines)
corpus = Reduce(c, lapply(corpora, CorpusToSentences))
CorpusToSentences = function(corpus) {
# Split every document in the corpus into sentences and return a new corpus
# with all the sentences as individual documents.
# Extract the text from each document in the corpus.
#text = lapply(corpus, "[[", "content")
# Basically convert the text
docs = lapply(corpus, ToSentences)
docs = as.vector(unlist(docs))
# Return a corpus with sentences as documents.
Corpus(VectorSource(docs))
}
corpus = Reduce(c, lapply(corpora, CorpusToSentences))
corpus
# Process the corpora contents.
corpus = tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
# Create a document term matrix from the corpus.
dtm = DocumentTermMatrix(corpus)
# Remove terms which are not contained in any of the documents.
dtm = dtm[ , colSums(as.matrix(dtm)) > 0]
# Load the sentiment lexicon (saved down in working directory as a
# comma separated value file).
lex = read.csv("inquirerbasic.csv", stringsAsFactors=FALSE)
# Collapse words with multiple entries into one entry. These are marked
# with a trailing #1, #2, ...
# Remove #1 tags.
lex$Entry = gsub("#1", "", lex$Entry)
# Remove entries that are still numbered (i.e. two or higher)
lex = lex[!grepl("#", lex$Entry), ]
# Extract the positive and negative words from the lexicon.
neg.lex = tolower(lex$Entry[lex$Negativ != ""])
pos.lex = tolower(lex$Entry[lex$Positiv != ""])
lex = read.csv("inquirerbasic.csv", stringsAsFactors=FALSE)
setwd("~/Desktop/dev/trump_speeches/")
lex = read.csv("inquirerbasic.csv", stringsAsFactors=FALSE)
lex$Entry = gsub("#1", "", lex$Entry)
lex = lex[!grepl("#", lex$Entry), ]
neg.lex = tolower(lex$Entry[lex$Negativ != ""])
pos.lex = tolower(lex$Entry[lex$Positiv != ""])
lex
neg.lex
terms
# Find the positive and negative terms using the lexicons.
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
# Specify positive terms which may be quiestionable.
pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
# Calculate the negative and positive sentence scores ("document scores").
neg.scores = rowSums(as.matrix(dtm[ , neg.terms]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
# Calulate the document signs ("sentence signs").
document.signs = sign(document.scores)
# Calculate the sentiment score
sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
neg.terms = terms[terms %in% neg.lex]
terms = colnames(dtm)
terms
neg.terms = terms[terms %in% neg.lex]
pos.terms = terms[terms %in% pos.lex]
pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
neg.scores = rowSums(as.matrix(dtm[ , neg.terms]))
pos.scores = rowSums(as.matrix(dtm[ , pos.terms]))
document.scores = pos.scores - neg.scores
document.signs = sign(document.scores)
sentiment.score = sum(document.signs == 1) / sum(document.signs !=0)
document.socres
document.scores
PosCloud = function() {
wordcloud(
pos.terms,
colSums(as.matrix(dtm[ , pos.terms])),
min.freq=1,
scale=c(4,0.7),
color=brewer.pal(n=9, "Blues")[6:9]
)
}
NegCloud = function() {
wordcloud(
neg.terms,
colSums(as.matrix(dtm[ , neg.terms])),
min.freq=1,
scale=c(4,0.7),
color=brewer.pal(n=9, "Reds")[6:9]
)
}
PosCloud
PosCloud()
NegCloud()
height = 8
width = 8
setwd("~/Desktop/dev/trump_speeches/analysis/")
pdf("PositiveCloud.pdf",width=width,height=height)
PosCloud()
dev.off()
pdf("NegativeCloud.pdf",width=width,height=height)
NegCloud()
dev.off()
document.scores
head(document.scores)
document
neg.scores
terms
dtm
dtm(head)
dtm(head)
dt
head(dtm)
dtm
help(dtm)
help(DocumentTermMatrix)
inspect(dtm[1:5])
dtm
document.scores
corpora
help(numrow)
help(numrows)
help(rownum)
speech_corpora <- length(txt_files)
speech_corpora
for (i in length(txt_files)) {
speech_corpora[i] = readLines(txt_files(i))
}
help(readLines)
txt_files[1]
setwd("~/Desktop/dev/trump_speeches/data")
speech_corpora[i] = readLines(txt_files(i))
for (i in length(txt_files)) {
speech_corpora[i] = readLines(con = txt_files[i])
}
for (i in length(txt_files)) {
readLines(con = txt_files[i])
}
speech_corpora = readLines(con = txt_files[i])
speech_corpora
speech_corpora[1]
for (i in length(txt_files)) {
speech_corpora[i] = readLines(con = txt_files[i])[2]
}
speech_corpora
speech_corpora[1]
speech_corpora[2]
speech_corpora[3]
speech_corpora[4]
speech_corpora[5]
for (i in length(txt_files)) {
f = readLines(con = txt_files[i])
speech_corpora[i] = f[2]
}
speech_corpora[4]
speech_corpora[4]
for (i in length(txt_files)) {
f = readLines(con = txt_files[i])
speech_corpora[i] = f
}
f = readLines(con = txt_files[i], header=TRUE)
for (i in length(txt_files)) {
speech_corpora = readLines(con = txt_files[i])
}
for (i in length(txt_files)) {
print(length(readLines(con = txt_files[i])))
}
txt_files
txt_files[1]
txt_files[2]
for (i in 1:length(txt_files)) {
print(length(readLines(con = txt_files[i])))
}
for (i in 1:length(txt_files)) {
speech_corpora[i] = readLines(con = txt_files[i])
}
warnings()
for (i in 1:length(txt_files)) {
speech_corpora[i] = readLines(con = txt_files[i])[2]
}
speech_corpora[1]
BreakDownCorpus = function(corpora) {
# Create a new corpus which merges existing corpora after splitting them
# into sentences.
corpus = Reduce(c, lapply(corpora, CorpusToSentences))
# Process the corpora contents.
corpus = tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
corpus
}
speech_corpi <- length(txt_files)
for (i in 1:length(txt_files)) {
speech_corpora[i] = readLines(con = txt_files[i])[2]
speech_corpi[i] = BreakDownCorpus(speech_corpus[i])
}
for (i in 1:length(txt_files)) {
speech_corpora[i] = readLines(con = txt_files[i])[2]
speech_corpi[i] = BreakDownCorpus(speech_corpora[i])
}
warnings()
corpus
help(VCorpus)
neg.terms.each <- length(txt_files)
pos.terms.each <- length(txt_files)
neg.scores.each <- length(txt_files)
pos.scores.each <- length(txt_files)
document.scores.each <- length(txt_files)
pos.scores.each <- length(txt_files)
document.scores.each <- length(txt_files)
document.signs.each <- length(txt_files)
sentiment.score.each <- length(txt_files)
for (i in 1:length(txt_files)) {
speech_corpi_temp = BreakDownCorpus(speech_corpora[i])
terms = colnames(speech_corpi_temp)
neg.terms.each[i] = terms[terms %in% neg.lex]
neg.terms.each[i] = terms[terms %in% pos.lex]
neg.scores.each[i] = rowSums(as.matrix(dtm[ , neg.terms.each[i]]))
pos.scores.each[i] = rowSums(as.matrix(dtm[ , pos.terms.each[i]]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each[i] = pos.scores.each[i] - neg.scores.each[i]
document.signs.each[i] = sign(document.scores)
sentiment.score.each[i] = sum(document.signs.each[i] == 1) / sum(document.signs.each[i] !=0)
}
for (i in 1:length(txt_files)) {
speech_corpi_temp = BreakDownCorpus(speech_corpora[i])
terms = colnames(speech_corpi_temp)
neg.terms.each[i] = terms[terms %in% neg.lex]
pos.terms.each[i] = terms[terms %in% pos.lex]
neg.scores.each[i] = rowSums(as.matrix(dtm[ , neg.terms.each[i]]))
pos.scores.each[i] = rowSums(as.matrix(dtm[ , pos.terms.each[i]]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each[i] = pos.scores.each[i] - neg.scores.each[i]
document.signs.each[i] = sign(document.scores)
sentiment.score.each[i] = sum(document.signs.each[i] == 1) / sum(document.signs.each[i] !=0)
}
for (i in 1:length(txt_files)) {
speech_corpi_temp = BreakDownCorpus(speech_corpora[i])
terms = colnames(speech_corpi_temp)
neg.terms.each[i] = terms[terms %in% neg.lex]
pos.terms.each[i] = terms[terms %in% pos.lex]
neg.scores.each[i] = rowSums(as.matrix(speech_corpi_temp[ , neg.terms.each[i]]))
pos.scores.each[i] = rowSums(as.matrix(speech_corpi_temp[ , pos.terms.each[i]]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each[i] = pos.scores.each[i] - neg.scores.each[i]
document.signs.each[i] = sign(document.scores)
sentiment.score.each[i] = sum(document.signs.each[i] == 1) / sum(document.signs.each[i] !=0)
}
terms
speech_corpi_temp
terms = colnames(speech_corpi_temp)
terms
speech_corpora[1]
for (i in 1:length(txt_files)) {
speech_corpi_temp = Reduce(c, lapply(speech_corpora[i], CorpusToSentences))
# Process the corpora contents.
speech_corpi_temp = tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE)
speech_corpi_temp = tm_map(corpus, content_transformer(tolower))
speech_corpi_temp = tm_map(corpus, removeWords, stopwords("english"))
speech_corpi_temp = tm_map(corpus, removeNumbers)
speech_corpi_temp = tm_map(corpus, stripWhitespace)
terms = colnames(speech_corpi_temp)
neg.terms.each[i] = terms[terms %in% neg.lex]
pos.terms.each[i] = terms[terms %in% pos.lex]
neg.scores.each[i] = rowSums(as.matrix(speech_corpi_temp[ , neg.terms.each[i]]))
pos.scores.each[i] = rowSums(as.matrix(speech_corpi_temp[ , pos.terms.each[i]]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each[i] = pos.scores.each[i] - neg.scores.each[i]
document.signs.each[i] = sign(document.scores)
sentiment.score.each[i] = sum(document.signs.each[i] == 1) / sum(document.signs.each[i] !=0)
}
corpora
for (i in 1:length(txt_files)) {
speech_corpi_temp = BreakDownCorpus(speech_corpora[i])
temp_dtm = DocumentTermMatrix(speech_corpi_temp)
temp_dtm = temp_dtm[ , colSums(as.matrix(temp_dtm)) > 0]
terms = colnames(speech_corpi_temp)
neg.terms.each[i] = terms[terms %in% neg.lex]
pos.terms.each[i] = terms[terms %in% pos.lex]
neg.scores.each[i] = rowSums(as.matrix(speech_corpi_temp[ , neg.terms.each[i]]))
pos.scores.each[i] = rowSums(as.matrix(speech_corpi_temp[ , pos.terms.each[i]]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each[i] = pos.scores.each[i] - neg.scores.each[i]
document.signs.each[i] = sign(document.scores)
sentiment.score.each[i] = sum(document.signs.each[i] == 1) / sum(document.signs.each[i] !=0)
}
for (i in 1:length(txt_files)) {
speech_corpi_temp = BreakDownCorpus(speech_corpora[i])
temp_dtm = DocumentTermMatrix(speech_corpi_temp)
temp_dtm = temp_dtm[ , colSums(as.matrix(temp_dtm)) > 0]
terms = colnames(speech_corpi_temp)
neg.terms.each[i] = terms[terms %in% neg.lex]
pos.terms.each[i] = terms[terms %in% pos.lex]
neg.scores.each[i] = rowSums(as.matrix(temp_dtm[ , neg.terms.each[i]]))
pos.scores.each[i] = rowSums(as.matrix(temp_dtm[ , pos.terms.each[i]]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each[i] = pos.scores.each[i] - neg.scores.each[i]
document.signs.each[i] = sign(document.scores)
sentiment.score.each[i] = sum(document.signs.each[i] == 1) / sum(document.signs.each[i] !=0)
}
neg.lex
terms
for (i in 1:length(txt_files)) {
speech_corpi_temp = BreakDownCorpus(speech_corpora[i])
temp_dtm = DocumentTermMatrix(speech_corpi_temp)
temp_dtm = temp_dtm[ , colSums(as.matrix(temp_dtm)) > 0]
terms = colnames(temp_dtm)
neg.terms.each[i] = terms[terms %in% neg.lex]
pos.terms.each[i] = terms[terms %in% pos.lex]
neg.scores.each[i] = rowSums(as.matrix(temp_dtm[ , neg.terms.each[i]]))
pos.scores.each[i] = rowSums(as.matrix(temp_dtm[ , pos.terms.each[i]]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each[i] = pos.scores.each[i] - neg.scores.each[i]
document.signs.each[i] = sign(document.scores)
sentiment.score.each[i] = sum(document.signs.each[i] == 1) / sum(document.signs.each[i] !=0)
}
warnings
warnings()
for (i in 1:length(txt_files)) {
speech_corpi_temp = BreakDownCorpus(speech_corpora[i])
temp_dtm = DocumentTermMatrix(speech_corpi_temp)
temp_dtm = temp_dtm[ , colSums(as.matrix(temp_dtm)) > 0]
terms = colnames(temp_dtm)
neg.terms.each = terms[terms %in% neg.lex]
pos.terms.each = terms[terms %in% pos.lex]
neg.scores.each = rowSums(as.matrix(temp_dtm[ , neg.terms.each]))
pos.scores.each = rowSums(as.matrix(temp_dtm[ , pos.terms.each]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each[i] = pos.scores.each - neg.scores.each
document.signs.each[i] = sign(document.scores)
sentiment.score.each[i] = sum(document.signs.each[i] == 1) / sum(document.signs.each[i] !=0)
}
warnings()
for (i in 1:length(txt_files)) {
speech_corpi_temp = BreakDownCorpus(speech_corpora[i])
temp_dtm = DocumentTermMatrix(speech_corpi_temp)
temp_dtm = temp_dtm[ , colSums(as.matrix(temp_dtm)) > 0]
terms = colnames(temp_dtm)
neg.terms.each = terms[terms %in% neg.lex]
pos.terms.each = terms[terms %in% pos.lex]
neg.scores.each = rowSums(as.matrix(temp_dtm[ , neg.terms.each]))
pos.scores.each = rowSums(as.matrix(temp_dtm[ , pos.terms.each]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each[i] = pos.scores.each - neg.scores.each
document.signs.each[i] = sign(document.scores.each[i])
sentiment.score.each[i] = sum(document.signs.each[i] == 1) / sum(document.signs.each[i] !=0)
}
warnings()
for (i in 1:length(txt_files)) {
speech_corpi_temp = BreakDownCorpus(speech_corpora[i])
temp_dtm = DocumentTermMatrix(speech_corpi_temp)
temp_dtm = temp_dtm[ , colSums(as.matrix(temp_dtm)) > 0]
terms = colnames(temp_dtm)
neg.terms.each = terms[terms %in% neg.lex]
pos.terms.each = terms[terms %in% pos.lex]
neg.scores.each = rowSums(as.matrix(temp_dtm[ , neg.terms.each]))
pos.scores.each = rowSums(as.matrix(temp_dtm[ , pos.terms.each]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each = pos.scores.each - neg.scores.each
document.signs.each = sign(document.scores.each)
sentiment.score.each[i] = sum(document.signs.each == 1) / sum(document.signs.each !=0)
}
sentiment.score.each
ggplot(sentiment.score.each, aes(Time, Sentiment)) + geom_line())
ggplot(sentiment.score.each, aes(Time, Sentiment)) + geom_line()
d <- data.frame(x = unlist(sentiment.score.each),
grp = rep(letters[1:length(sentiment.score.each)],
times = sapply(sentiment.score.each,length)))
ggplot(d, aes(Time, Sentiment)) + geom_line()
ggplot(d, aes(x = grp, y = x)) + geom_line()
d
d <- data.frame(x = unlist(sentiment.score.each),
grp = rep(1:length(sentiment.score.each),
times = sapply(sentiment.score.each,length)))
d
ggplot(d, aes(x = grp, y = x)) + geom_line()
pdf("SentimentPlot.pdf",width=9,height=6)
ggplot(d, aes(x = grp, y = x)) + geom_line()
dev.off()
speech_corpora[50]
txt_files
for (i in 1:length(txt_files)) { print txt_files[i] }
for (i in 1:length(txt_files)) { print txt_files[i] }
for (i in 1:length(txt_files)) {
print(txt_files[i])
}
speech_corpora[53]
sort.list(txt_files)
txt_files
packs = c(
"tm",                         # Text mining
"tm.plugin.webmining",        # Web-source plugin for text mining
"SnowballC",                  # Stemmer
"RColorBrewer",               # Colors for visualisation
"ggplot2",                    # Plotting
"wordcloud",                  # Draw wordclouds
"openNLP",                     # Split text into sentences.
"gtools"
)
sapply(packs, require, character.only=TRUE)   # Load the packages.
mixedsort(txt_files)
txt_files = list.files(pattern = 'speech_')
txt_files = mixedsort(txt_files)
corpora = lapply(txt_files, readLines)
BreakDownCorpus(corpora)
setwd("~/Desktop/dev/trump_speeches/analysis/")
setwd("~/Desktop/dev/trump_speeches/data")
speech_corpora <- length(txt_files)
speech_corpi <- length(txt_files)
for (i in 1:length(txt_files)) {
speech_corpora[i] = readLines(con = txt_files[i])[2]
}
for (i in 1:length(txt_files)) {
speech_corpi_temp = BreakDownCorpus(speech_corpora[i])
temp_dtm = DocumentTermMatrix(speech_corpi_temp)
temp_dtm = temp_dtm[ , colSums(as.matrix(temp_dtm)) > 0]
terms = colnames(temp_dtm)
neg.terms.each = terms[terms %in% neg.lex]
pos.terms.each = terms[terms %in% pos.lex]
neg.scores.each = rowSums(as.matrix(temp_dtm[ , neg.terms.each]))
pos.scores.each = rowSums(as.matrix(temp_dtm[ , pos.terms.each]))
# Specify positive terms which may be quiestionable.
#pos.terms.adj = setdiff(pos.terms, c("equity", "share", "consensus"))
document.scores.each = pos.scores.each - neg.scores.each
document.signs.each = sign(document.scores.each)
sentiment.score.each[i] = sum(document.signs.each == 1) / sum(document.signs.each !=0)
}
d <- data.frame(x = unlist(sentiment.score.each),
grp = rep(1:length(sentiment.score.each),
times = sapply(sentiment.score.each,length)))
pdf("SentimentPlot.pdf",width=9,height=6)
ggplot(d, aes(x = grp, y = x)) + geom_line()
dev.off()
sentiment.score.each
txt_files
